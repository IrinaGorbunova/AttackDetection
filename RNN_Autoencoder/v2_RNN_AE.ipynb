{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "v2_RNN_AE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19de1a631b4f4c6bba48086beaf84c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a11ed0269d554ade8afa3347a52aeeb9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_53c95315128e4ed89fb0c55d87258d88",
              "IPY_MODEL_cba3576483b94638818d2c3b6218a751"
            ]
          }
        },
        "a11ed0269d554ade8afa3347a52aeeb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53c95315128e4ed89fb0c55d87258d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e388f5b61a5e4a8f9e3a0f599cc0a6d2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b302f7794a444f9b47748ab0eb2d928"
          }
        },
        "cba3576483b94638818d2c3b6218a751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e20352dfde94705b7b2b62e9cff4f33",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:14&lt;00:00, 6.09MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef8f6231280b4cc9a7aa8787ec9ff2e1"
          }
        },
        "e388f5b61a5e4a8f9e3a0f599cc0a6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b302f7794a444f9b47748ab0eb2d928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e20352dfde94705b7b2b62e9cff4f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef8f6231280b4cc9a7aa8787ec9ff2e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSmXB9NX7JGM"
      },
      "source": [
        "# RNN-Autoencoder\n",
        "\n",
        "Попытка использовать большое количество оригиналов для обучения без учителя. С этой целью используется модель автоэнкодера с RNN энкодером и декодером. В качестве входных данных эмбеддинги, полученные с помощью ResNet 34.\n",
        "\n",
        "Гипотеза: При реконструкции атак ошибка должна быть выше (аномалии).\n",
        "\n",
        "Улучшение: Предобученная ResNet 34 (например, часть CNN из CNN_Transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj-7Q2vvDZl-"
      },
      "source": [
        "## Dependences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2H3pD4GrRKN",
        "outputId": "74c239a9-825e-4bf1-9264-e17078919439"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 28 21:59:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     6W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLbLrZhUnbhs",
        "outputId": "e967738c-eee9-4ed1-a2ec-ca1cdb3f98de"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF8XH99LcxmV"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import ffmpeg\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "from random import choice\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxKHcYo71lRV"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9PZXpS79DWk"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TsKdzsuDgRB"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etC2F6MAXhlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ab7a8d-1571-439e-dc8b-35a2f473774b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1WHKFeTzJfL"
      },
      "source": [
        "# %%time\n",
        "# !gdown --id 1SB0qwhhlEFH1DZNeaFrsGEbYfWerRxWc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3CHoOXNVubv",
        "outputId": "d4c1507b-917e-4b60-b3b7-dde9b1487f4f"
      },
      "source": [
        "%%time\n",
        "!tar xzf '/content/drive/MyDrive/combined_ds.tar.gz' #combined_ds.tar.gz #"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.49 s, sys: 228 ms, total: 1.72 s\n",
            "Wall time: 5min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jryH63puAv9",
        "outputId": "1590436c-eab1-46f3-e5bd-142d2acb2adf"
      },
      "source": [
        "dir_1 = '/content/irina/attack/opt/labeler/static/video/api'\n",
        "images_1 = glob.glob(dir_1 + '/*.*') + glob.glob(dir_1 + '/*/*.*')\n",
        "len(images_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuBjq2Iqwynz",
        "outputId": "5fef8407-dea3-4060-ca3d-5af3ed15b704"
      },
      "source": [
        "dir_2 = '/content/irina/original/opt/labeler/static/video/api'\n",
        "images_2 = glob.glob(dir_2 + '/*.*') + glob.glob(dir_2 + '/*/*.*')\n",
        "len(images_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT-EcMSA6F-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7ce215-4bfb-433f-85b0-4206af5bcee6"
      },
      "source": [
        "train_images = images_2[:-48]\n",
        "print(len(train_images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8iYtxpv4HGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7479ad-b393-422c-c4fc-b22d1de23b29"
      },
      "source": [
        "val_images = images_2[-48:]\n",
        "print(len(val_images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Ksk2_BZzrK",
        "outputId": "23b5466e-912c-47fd-bd7d-4b4a36f746af"
      },
      "source": [
        "val_images_1 = images_1\n",
        "print(len(val_images_1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3TFupB2Dnk7"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ZRoG55XqHD"
      },
      "source": [
        "h, w = 224, 224\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYvEHxN0saVf"
      },
      "source": [
        "def check_rotation(path_video_file):\n",
        "\n",
        "    meta_dict = ffmpeg.probe(path_video_file)\n",
        "\n",
        "    rotateCode = None\n",
        "    try:\n",
        "        if int(meta_dict['streams'][0]['tags']['rotate']) == 90:\n",
        "            rotateCode = cv2.ROTATE_90_CLOCKWISE\n",
        "        elif int(meta_dict['streams'][0]['tags']['rotate']) == 180:\n",
        "            rotateCode = cv2.ROTATE_180\n",
        "        elif int(meta_dict['streams'][0]['tags']['rotate']) == 270:\n",
        "            rotateCode = cv2.ROTATE_90_COUNTERCLOCKWISE\n",
        "    except: \n",
        "        pass\n",
        "        \n",
        "    return rotateCode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C7Fyc_bsc_V"
      },
      "source": [
        "def correct_rotation(frame, rotateCode):  \n",
        "    return cv2.rotate(frame, rotateCode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcigkDg4cz_7"
      },
      "source": [
        "def get_frames(filename, n_max=float('inf')):\n",
        "    \n",
        "    frames = []\n",
        "    v_cap = cv2.VideoCapture(filename)\n",
        "    rotateCode = check_rotation(filename)\n",
        "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    n_frames = min(v_len, n_max)\n",
        "    frame_list= np.linspace(0, v_len-1, n_frames, dtype=np.int16)\n",
        "\n",
        "    for fn in range(v_len):\n",
        "        success, frame = v_cap.read()\n",
        "        if success is False:\n",
        "            continue\n",
        "        if (fn in frame_list):\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            if rotateCode is not None:\n",
        "                frame = correct_rotation(frame, rotateCode)  \n",
        "            frames.append(frame)\n",
        "\n",
        "    v_cap.release()\n",
        "\n",
        "    return frames, len(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZunZ2svjYKM-"
      },
      "source": [
        "def transform_frames(frames, train=True):\n",
        "\n",
        "    img_transforms_0 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    img_transforms_1 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.Grayscale(num_output_channels=3),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    img_transforms_2 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.RandomHorizontalFlip(p=1.),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    img_transforms_3 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.Grayscale(num_output_channels=3),\n",
        "                transforms.RandomHorizontalFlip(p=1.),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    \n",
        "    img_transforms = [img_transforms_0, \n",
        "                      img_transforms_1, \n",
        "                      img_transforms_2, \n",
        "                      img_transforms_3] \n",
        "    if train:\n",
        "        img_transform = random.choice(img_transforms)\n",
        "    else:\n",
        "        img_transform = img_transforms_0\n",
        "    \n",
        "    frames_tr = []\n",
        "    for frame in frames:\n",
        "        frame = Image.fromarray(frame)       \n",
        "        frame_tr = img_transform(frame)\n",
        "        frames_tr.append(frame_tr)\n",
        "        \n",
        "    imgs_tensor = torch.stack(frames_tr)    \n",
        "\n",
        "    return imgs_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ES92OoTDy8d"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3b9HKWICeK"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, \n",
        "                            dropout=0.2, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "\n",
        "        packed_frames_emb = nn.utils.rnn.pack_padded_sequence(x, lens, enforce_sorted=False)\n",
        "        packed_out, h = self.lstm(packed_frames_emb)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, num_layers, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(hidden_size, output_size, num_layers, batch_first=True,\n",
        "                            dropout=0.2, bidirectional=bidirectional)\n",
        "        \n",
        "    def forward(self, x, lens):\n",
        "\n",
        "        packed_frames_emb = nn.utils.rnn.pack_padded_sequence(x, lens, enforce_sorted=False)\n",
        "        packed_out, h = self.lstm(packed_frames_emb)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class AutoEncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=False):\n",
        "        super(AutoEncoderRNN, self).__init__()\n",
        "        self.encoder = EncoderRNN(input_size, hidden_size, num_layers, bidirectional)\n",
        "        self.decoder = DecoderRNN(hidden_size, input_size, num_layers, bidirectional)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        encoded_x = self.encoder(x, lens)\n",
        "        decoded_x = self.decoder(encoded_x, lens)\n",
        "\n",
        "        return decoded_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9_de_h6XqbJ"
      },
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        \n",
        "        resnet = models.resnet34(pretrained=pretrained) \n",
        "\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        # (batch_size, 512, 1, 1)\n",
        "        out = self.resnet(image)\n",
        "\n",
        "        return out.view(-1, 512) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGahhMAvR2BY"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(\n",
        "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.scale * self.pe[:x.size(0), :]\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class CNN_Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, nlayers, hidden, nhead, dim_feedforward, \n",
        "                 dropout=0, activation='relu'):\n",
        "\n",
        "        super(CNN_Transformer, self).__init__()\n",
        "\n",
        "        self.hidden = hidden\n",
        "        \n",
        "        # CNN\n",
        "        self.cnn = CNNEncoder()\n",
        "        self.conv = nn.Conv2d(512, hidden, 1)\n",
        "\n",
        "        # Transformer\n",
        "        self.pos = PositionalEncoding(hidden) \n",
        "        encoder_layer = nn.TransformerEncoderLayer(hidden, \n",
        "                                                   nhead, \n",
        "                                                   dim_feedforward, \n",
        "                                                   dropout, \n",
        "                                                   activation)\n",
        "        encoder_norm = nn.LayerNorm(hidden)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, \n",
        "                                                 nlayers, \n",
        "                                                 encoder_norm)\n",
        "\n",
        "        # Classifier\n",
        "        self.dropout= nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden*1, num_classes)\n",
        "\n",
        "    def forward(self, frames, f_lens, mask):\n",
        "        \n",
        "        bs, s, c, height, width = frames.shape\n",
        "        frames_emb = torch.zeros(s, bs, self.hidden).to(device)\n",
        "\n",
        "        for i in range(s):\n",
        "            img_emb = self.cnn(frames[:, i])\n",
        "            img_emb = torch.relu(self.conv(img_emb))\n",
        "            frames_emb[i] = img_emb.view(bs, -1)\n",
        "\n",
        "        frames_emb = self.pos(frames_emb) \n",
        "        out = self.transformer(src=frames_emb, src_key_padding_mask=mask)\n",
        "        out = self.fc(self.dropout(torch.max(out, 0)[0]))\n",
        " \n",
        "        return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFHGppF0Ij3g"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "19de1a631b4f4c6bba48086beaf84c38",
            "a11ed0269d554ade8afa3347a52aeeb9",
            "53c95315128e4ed89fb0c55d87258d88",
            "cba3576483b94638818d2c3b6218a751",
            "e388f5b61a5e4a8f9e3a0f599cc0a6d2",
            "1b302f7794a444f9b47748ab0eb2d928",
            "9e20352dfde94705b7b2b62e9cff4f33",
            "ef8f6231280b4cc9a7aa8787ec9ff2e1"
          ]
        },
        "id": "2mUJOtFzPhFN",
        "outputId": "f83dcf20-68fb-4e88-b174-3b8ad43639da"
      },
      "source": [
        "cnn = CNNEncoder()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19de1a631b4f4c6bba48086beaf84c38",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtzgQO-PkIK"
      },
      "source": [
        "cnn = cnn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcEnjU-7SoEv"
      },
      "source": [
        "model_clf = CNN_Transformer(num_classes=2,\n",
        "                                nlayers=1,\n",
        "                                hidden=32,\n",
        "                                nhead=4,\n",
        "                                dim_feedforward=128,\n",
        "                                dropout=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDxBAlKUSsjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1780873a-96ae-465f-e1b1-049d03440adb"
      },
      "source": [
        "model_clf.load_state_dict(torch.load('/content/v2_CNN_Transformer_best_score.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll1wbmgyS3Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc1f67b-6acb-4ff3-a726-580896504ce9"
      },
      "source": [
        "cnn.load_state_dict(model_clf.cnn.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzP5gMucdnNt"
      },
      "source": [
        "cnn = cnn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2OZO2gAIYZd"
      },
      "source": [
        "model = AutoEncoderRNN(input_size=512, hidden_size=32, num_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvr30rzlNd1x"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_jSkZ93Nd1x"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 0.001)\n",
        "criterion = nn.MSELoss(reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGyRTi6eNd1y"
      },
      "source": [
        "max_epochs = 100\n",
        "every_epochs = 10\n",
        "n_max = 30\n",
        "clip = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfJJL6_GGCgf"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416eecDSwLNh"
      },
      "source": [
        "def generate_batch(images, batch_size, train=True, n_max=30):\n",
        "    \n",
        "    if train:\n",
        "        random_index = np.random.randint(0, len(images), size=batch_size)\n",
        "        np.random.shuffle(random_index)\n",
        "        filenames = np.array(images)[random_index]\n",
        "    else:\n",
        "        filenames = np.array(images)\n",
        "\n",
        "    batch_images = []\n",
        "    f_lens = []\n",
        "\n",
        "    for filename in filenames:\n",
        "        frames, f_len = get_frames(filename, n_max=n_max)\n",
        "        f_lens.append(f_len)\n",
        "        img_tensor = transform_frames(frames, train)\n",
        "        batch_images.append(img_tensor)\n",
        "\n",
        "    max_len = np.max(f_lens)\n",
        "\n",
        "    _, c, h, w = batch_images[0].shape\n",
        "    batch_images_padded = torch.zeros((batch_size, max_len, c, h, w)) \n",
        "    mask = [] \n",
        "\n",
        "    for i, img in enumerate(batch_images):\n",
        "\n",
        "        batch_images_padded[i, :f_lens[i], :, :, :] = img\n",
        "    \n",
        "    return batch_images_padded, torch.LongTensor(f_lens) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5OpYK0ryqMc"
      },
      "source": [
        "def evaluate(model, criterion, val_images, batch_size):\n",
        "    \n",
        "    model.eval()\n",
        "    cnn.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    outputs = []\n",
        "\n",
        "    val_size = len(val_images) // batch_size * batch_size\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in range(0, val_size, batch_size):\n",
        "\n",
        "            batch = generate_batch(val_images[i:i+batch_size], batch_size=batch_size, train=False)\n",
        "            frames = batch[0].to(device)\n",
        "\n",
        "            # Get embeddings\n",
        "            with torch.no_grad():\n",
        "                bs, s, c, height, width = frames.shape\n",
        "                frames_emb = torch.zeros(s, bs, 512).to(device)\n",
        "                for i in range(s):\n",
        "                    frames_emb[i] = cnn(frames[:, i])\n",
        "        \n",
        "            output = model(frames_emb, batch[1]) \n",
        "\n",
        "            loss_ = criterion(output, frames_emb)\n",
        "            loss_batch = []\n",
        "            for i in range(bs):\n",
        "                loss_batch.append(loss_[:batch[1][i], i].mean())\n",
        "\n",
        "            loss = torch.mean(torch.stack(loss_batch)) \n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / (val_size // batch_size) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJC-1g2sNd1y",
        "outputId": "75a01ded-2321-479a-9cac-06621f2aef09"
      },
      "source": [
        "%%time\n",
        "epoch_loss = 0\n",
        "cnn.eval()\n",
        "\n",
        "for epoch in range(1, max_epochs+1):\n",
        "\n",
        "    batch = generate_batch(images=train_images, \n",
        "                           batch_size=8,  \n",
        "                           train=True, \n",
        "                           n_max=n_max)\n",
        "    frames = batch[0].to(device)\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        bs, s, c, height, width = frames.shape\n",
        "        frames_emb = torch.zeros(s, bs, 512).to(device)\n",
        "        for i in range(s):\n",
        "            frames_emb[i] = cnn(frames[:, i])\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "        \n",
        "    outputs = model(frames_emb, batch[1]) \n",
        "\n",
        "    loss_ = criterion(outputs, frames_emb)\n",
        "    loss_batch = []\n",
        "    for i in range(bs):\n",
        "        loss_batch.append(loss_[:batch[1][i], i].mean())\n",
        "\n",
        "    loss = torch.mean(torch.stack(loss_batch))        \n",
        "    loss.backward()       \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)       \n",
        "    optimizer.step()       \n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    if epoch % every_epochs == 0:\n",
        "        val_loss_0 = evaluate(model, criterion, val_images, batch_size=16)\n",
        "        val_loss_1 = evaluate(model, criterion, val_images_1, batch_size=16)   \n",
        "        print('Epoch : {}'.format(epoch))\n",
        "        print('Train loss : {}'.format(epoch_loss / every_epochs))\n",
        "        print('Val loss 0 : {}'.format(val_loss_0))\n",
        "        print('Val loss 1 : {}'.format(val_loss_1))\n",
        "\n",
        "        epoch_loss = 0\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 10\n",
            "Train loss : 0.8576911628246308\n",
            "Val loss 0 : 0.5643331011136373\n",
            "Val loss 1 : 1.4982965389887493\n",
            "\n",
            "Epoch : 20\n",
            "Train loss : 0.4660246282815933\n",
            "Val loss 0 : 0.44582189122835797\n",
            "Val loss 1 : 1.6527654727300007\n",
            "\n",
            "Epoch : 30\n",
            "Train loss : 0.4107699304819107\n",
            "Val loss 0 : 0.4136157234509786\n",
            "Val loss 1 : 1.5996062755584717\n",
            "\n",
            "Epoch : 40\n",
            "Train loss : 0.41363338232040403\n",
            "Val loss 0 : 0.39356998602549237\n",
            "Val loss 1 : 1.5426439841588337\n",
            "\n",
            "Epoch : 50\n",
            "Train loss : 0.35441747307777405\n",
            "Val loss 0 : 0.3842362662156423\n",
            "Val loss 1 : 1.5570050080617268\n",
            "\n",
            "Epoch : 60\n",
            "Train loss : 0.33060345947742464\n",
            "Val loss 0 : 0.3825668195883433\n",
            "Val loss 1 : 1.5792627334594727\n",
            "\n",
            "Epoch : 70\n",
            "Train loss : 0.36366947889328005\n",
            "Val loss 0 : 0.37882551550865173\n",
            "Val loss 1 : 1.5033152898152669\n",
            "\n",
            "Epoch : 80\n",
            "Train loss : 0.33928095996379853\n",
            "Val loss 0 : 0.37560002009073895\n",
            "Val loss 1 : 1.5995275576909382\n",
            "\n",
            "Epoch : 90\n",
            "Train loss : 0.3522491306066513\n",
            "Val loss 0 : 0.37582847476005554\n",
            "Val loss 1 : 1.5578138430913289\n",
            "\n",
            "Epoch : 100\n",
            "Train loss : 0.356269970536232\n",
            "Val loss 0 : 0.3749303420384725\n",
            "Val loss 1 : 1.5555814901987712\n",
            "\n",
            "CPU times: user 20min 37s, sys: 2min 35s, total: 23min 12s\n",
            "Wall time: 19min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv4c4P07y9lK"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/v2_RNN_AE.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI7pbMUUnDXZ"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWJRVsTUnWWS"
      },
      "source": [
        "def predict(model, criterion, val_images, batch_size=1):\n",
        "    \n",
        "    model.eval()\n",
        "    cnn.eval()\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    val_size = len(val_images) // batch_size * batch_size\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in range(0, val_size, batch_size):\n",
        "\n",
        "            batch = generate_batch(val_images[i:i+batch_size], batch_size=batch_size, train=False)\n",
        "            frames = batch[0].to(device)\n",
        "            f_len = batch[1]\n",
        "\n",
        "            # Get embeddings\n",
        "            with torch.no_grad():\n",
        "                bs, s, c, height, width = frames.shape\n",
        "                frames_emb = torch.zeros(s, bs, 512).to(device)\n",
        "                for i in range(s):\n",
        "                    frames_emb[i] = cnn(frames[:, i])\n",
        "        \n",
        "            output = model(frames_emb, f_len) \n",
        "\n",
        "            loss_ = criterion(output, frames_emb)\n",
        "            loss_batch = []\n",
        "            for i in range(bs):\n",
        "                loss_batch.append(loss_[:f_len[i], i].mean())\n",
        "\n",
        "            losses += loss_batch \n",
        "        \n",
        "    return losses "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBJACbC76l4c"
      },
      "source": [
        "out_0 = predict(model, criterion, val_images, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG2NqUQ0ydCP"
      },
      "source": [
        "out_1 = predict(model, criterion, val_images_1, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22V2lMM11fr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cd2c50c7-bb90-43fb-ac83-d847cce8b102"
      },
      "source": [
        "plt.figure()\n",
        "plt.eventplot(out_0, orientation='horizontal', colors='b')\n",
        "plt.eventplot(out_1, orientation='horizontal', colors='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajwxuOSSzW-1"
      },
      "source": [
        "out_0 = predict(model, criterion, val_images[-8:], batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8GudkR2zW-4"
      },
      "source": [
        "out_1 = predict(model, criterion, val_images_1[-8:], batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2z3-WOQHzW-4",
        "outputId": "fb1ce9bc-a26c-421a-f393-e217b67b62b6"
      },
      "source": [
        "plt.figure()\n",
        "plt.eventplot(out_0, orientation='horizontal', colors='b')\n",
        "plt.eventplot(out_1, orientation='horizontal', colors='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mMG7yuynWwY"
      },
      "source": [
        "Вывод: По значению ошибки реконструкции оригиналы и атаки довольно хорошо разделяются."
      ]
    }
  ]
}