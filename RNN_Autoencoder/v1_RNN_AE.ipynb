{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rnn_6_plus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d92c74d8415242df884591055ced217b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04a42245a045489f9857499dfe1e6a52",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d5d6af8185b8469f8c3b59b70839b972",
              "IPY_MODEL_67950f61ecef4991b109f5ba516a8069"
            ]
          }
        },
        "04a42245a045489f9857499dfe1e6a52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5d6af8185b8469f8c3b59b70839b972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_968c1cc91275427c9e9e3079e0785eb6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7600930669142c99b93383779a1cc85"
          }
        },
        "67950f61ecef4991b109f5ba516a8069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02f9d1af01834b559cd7a6f600d3f5d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [05:34&lt;00:00, 261kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6993d89c650d453cad1205e9c47b2846"
          }
        },
        "968c1cc91275427c9e9e3079e0785eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7600930669142c99b93383779a1cc85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02f9d1af01834b559cd7a6f600d3f5d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6993d89c650d453cad1205e9c47b2846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSmXB9NX7JGM"
      },
      "source": [
        "# RNN-Autoencoder\n",
        "\n",
        "Попытка использовать большое количество оригиналов для обучения без учителя. С этой целью используется модель автоэнкодера с RNN энкодером и декодером. В качестве входных данных эмбеддинги, полученные с помощью ResNet 34.\n",
        "\n",
        "Гипотеза: При реконструкции атак ошибка должна быть выше (аномалии)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj-7Q2vvDZl-"
      },
      "source": [
        "## Dependences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2H3pD4GrRKN",
        "outputId": "00591172-2545-4ae2-9f24-76fc34ef6c85"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 28 22:39:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLbLrZhUnbhs",
        "outputId": "aab7b564-5dac-486f-bce2-1495c4c5402e"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF8XH99LcxmV"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import ffmpeg\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "from random import choice\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxKHcYo71lRV"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9PZXpS79DWk"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TsKdzsuDgRB"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etC2F6MAXhlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d083b003-bfa9-4e91-df9d-e642d52a7912"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1WHKFeTzJfL"
      },
      "source": [
        "# %%time\n",
        "# !gdown --id 1SB0qwhhlEFH1DZNeaFrsGEbYfWerRxWc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3CHoOXNVubv",
        "outputId": "2db58ba9-c9da-40aa-f525-8615a8aed09c"
      },
      "source": [
        "%%time\n",
        "!tar xzf '/content/drive/MyDrive/combined_ds.tar.gz' #combined_ds.tar.gz #"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.48 s, sys: 200 ms, total: 1.68 s\n",
            "Wall time: 5min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jryH63puAv9",
        "outputId": "e1eae77d-88a7-48af-a038-98044b7c94de"
      },
      "source": [
        "dir_1 = '/content/irina/attack/opt/labeler/static/video/api'\n",
        "images_1 = glob.glob(dir_1 + '/*.*') + glob.glob(dir_1 + '/*/*.*')\n",
        "len(images_1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuBjq2Iqwynz",
        "outputId": "124e17df-1fbe-47aa-e5e2-fb7efa86f393"
      },
      "source": [
        "dir_2 = '/content/irina/original/opt/labeler/static/video/api'\n",
        "images_2 = glob.glob(dir_2 + '/*.*') + glob.glob(dir_2 + '/*/*.*')\n",
        "len(images_2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT-EcMSA6F-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd00ef89-01d2-4617-eb7e-70a755e25014"
      },
      "source": [
        "train_images = images_2[:-48]\n",
        "print(len(train_images))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8iYtxpv4HGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5c2c65-3980-4bb3-af85-a78d44a88421"
      },
      "source": [
        "val_images = images_2[-48:]\n",
        "print(len(val_images))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Ksk2_BZzrK",
        "outputId": "5dd0be2c-8be4-489f-b3f3-3e37934664e8"
      },
      "source": [
        "val_images_1 = images_1\n",
        "print(len(val_images_1))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3TFupB2Dnk7"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ZRoG55XqHD"
      },
      "source": [
        "h, w = 224, 224\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYvEHxN0saVf"
      },
      "source": [
        "def check_rotation(path_video_file):\n",
        "\n",
        "    meta_dict = ffmpeg.probe(path_video_file)\n",
        "\n",
        "    rotateCode = None\n",
        "    try:\n",
        "        if int(meta_dict['streams'][0]['tags']['rotate']) == 90:\n",
        "            rotateCode = cv2.ROTATE_90_CLOCKWISE\n",
        "        elif int(meta_dict['streams'][0]['tags']['rotate']) == 180:\n",
        "            rotateCode = cv2.ROTATE_180\n",
        "        elif int(meta_dict['streams'][0]['tags']['rotate']) == 270:\n",
        "            rotateCode = cv2.ROTATE_90_COUNTERCLOCKWISE\n",
        "    except: \n",
        "        pass\n",
        "        \n",
        "    return rotateCode"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C7Fyc_bsc_V"
      },
      "source": [
        "def correct_rotation(frame, rotateCode):  \n",
        "    return cv2.rotate(frame, rotateCode)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcigkDg4cz_7"
      },
      "source": [
        "def get_frames(filename, n_max=float('inf')):\n",
        "    \n",
        "    frames = []\n",
        "    v_cap = cv2.VideoCapture(filename)\n",
        "    rotateCode = check_rotation(filename)\n",
        "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    n_frames = min(v_len, n_max)\n",
        "    frame_list= np.linspace(0, v_len-1, n_frames, dtype=np.int16)\n",
        "\n",
        "    for fn in range(v_len):\n",
        "        success, frame = v_cap.read()\n",
        "        if success is False:\n",
        "            continue\n",
        "        if (fn in frame_list):\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            if rotateCode is not None:\n",
        "                frame = correct_rotation(frame, rotateCode)  \n",
        "            frames.append(frame)\n",
        "\n",
        "    v_cap.release()\n",
        "\n",
        "    return frames, len(frames)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZunZ2svjYKM-"
      },
      "source": [
        "def transform_frames(frames, train=True):\n",
        "\n",
        "    img_transforms_0 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    img_transforms_1 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.Grayscale(num_output_channels=3),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    img_transforms_2 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.RandomHorizontalFlip(p=1.),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    img_transforms_3 = transforms.Compose([\n",
        "                transforms.Resize((h,w)),\n",
        "                transforms.Grayscale(num_output_channels=3),\n",
        "                transforms.RandomHorizontalFlip(p=1.),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)])\n",
        "    \n",
        "    img_transforms = [img_transforms_0, \n",
        "                      img_transforms_1, \n",
        "                      img_transforms_2, \n",
        "                      img_transforms_3] \n",
        "    if train:\n",
        "        img_transform = random.choice(img_transforms)\n",
        "    else:\n",
        "        img_transform = img_transforms_0\n",
        "    \n",
        "    frames_tr = []\n",
        "    for frame in frames:\n",
        "        frame = Image.fromarray(frame)       \n",
        "        frame_tr = img_transform(frame)\n",
        "        frames_tr.append(frame_tr)\n",
        "        \n",
        "    imgs_tensor = torch.stack(frames_tr)    \n",
        "\n",
        "    return imgs_tensor"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ES92OoTDy8d"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3b9HKWICeK"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, \n",
        "                            dropout=0.2, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "\n",
        "        packed_frames_emb = nn.utils.rnn.pack_padded_sequence(x, lens, enforce_sorted=False)\n",
        "        packed_out, h = self.lstm(packed_frames_emb)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, num_layers, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(hidden_size, output_size, num_layers, batch_first=True,\n",
        "                            dropout=0.2, bidirectional=bidirectional)\n",
        "        \n",
        "    def forward(self, x, lens):\n",
        "\n",
        "        packed_frames_emb = nn.utils.rnn.pack_padded_sequence(x, lens, enforce_sorted=False)\n",
        "        packed_out, h = self.lstm(packed_frames_emb)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class AutoEncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=False):\n",
        "        super(AutoEncoderRNN, self).__init__()\n",
        "        self.encoder = EncoderRNN(input_size, hidden_size, num_layers, bidirectional)\n",
        "        self.decoder = DecoderRNN(hidden_size, input_size, num_layers, bidirectional)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        encoded_x = self.encoder(x, lens)\n",
        "        decoded_x = self.decoder(encoded_x, lens)\n",
        "\n",
        "        return decoded_x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9_de_h6XqbJ"
      },
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        \n",
        "        resnet = models.resnet34(pretrained=pretrained) \n",
        "\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        # (batch_size, 512, 1, 1)\n",
        "        out = self.resnet(image)\n",
        "\n",
        "        return out.view(-1, 512) "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFHGppF0Ij3g"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d92c74d8415242df884591055ced217b",
            "04a42245a045489f9857499dfe1e6a52",
            "d5d6af8185b8469f8c3b59b70839b972",
            "67950f61ecef4991b109f5ba516a8069",
            "968c1cc91275427c9e9e3079e0785eb6",
            "b7600930669142c99b93383779a1cc85",
            "02f9d1af01834b559cd7a6f600d3f5d1",
            "6993d89c650d453cad1205e9c47b2846"
          ]
        },
        "id": "2mUJOtFzPhFN",
        "outputId": "78d24eb9-4bd7-4b7b-a7ab-f7a0d330faf4"
      },
      "source": [
        "cnn = CNNEncoder()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d92c74d8415242df884591055ced217b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtzgQO-PkIK"
      },
      "source": [
        "cnn = cnn.to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2OZO2gAIYZd"
      },
      "source": [
        "model = AutoEncoderRNN(input_size=512, hidden_size=32, num_layers=2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvr30rzlNd1x"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_jSkZ93Nd1x"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 0.001)\n",
        "criterion = nn.MSELoss(reduction='none')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGyRTi6eNd1y"
      },
      "source": [
        "max_epochs = 100\n",
        "every_epochs = 10\n",
        "n_max = 30\n",
        "clip = 3"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfJJL6_GGCgf"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416eecDSwLNh"
      },
      "source": [
        "def generate_batch(images, batch_size, train=True, n_max=30):\n",
        "    \n",
        "    if train:\n",
        "        random_index = np.random.randint(0, len(images), size=batch_size)\n",
        "        np.random.shuffle(random_index)\n",
        "        filenames = np.array(images)[random_index]\n",
        "    else:\n",
        "        filenames = np.array(images)\n",
        "\n",
        "    batch_images = []\n",
        "    f_lens = []\n",
        "\n",
        "    for filename in filenames:\n",
        "        frames, f_len = get_frames(filename, n_max=n_max)\n",
        "        f_lens.append(f_len)\n",
        "        img_tensor = transform_frames(frames, train)\n",
        "        batch_images.append(img_tensor)\n",
        "\n",
        "    max_len = np.max(f_lens)\n",
        "\n",
        "    _, c, h, w = batch_images[0].shape\n",
        "    batch_images_padded = torch.zeros((batch_size, max_len, c, h, w)) \n",
        "    mask = [] \n",
        "\n",
        "    for i, img in enumerate(batch_images):\n",
        "\n",
        "        batch_images_padded[i, :f_lens[i], :, :, :] = img\n",
        "    \n",
        "    return batch_images_padded, torch.LongTensor(f_lens) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5OpYK0ryqMc"
      },
      "source": [
        "def evaluate(model, criterion, val_images, batch_size):\n",
        "    \n",
        "    model.eval()\n",
        "    cnn.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    outputs = []\n",
        "\n",
        "    val_size = len(val_images) // batch_size * batch_size\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in range(0, val_size, batch_size):\n",
        "\n",
        "            batch = generate_batch(val_images[i:i+batch_size], batch_size=batch_size, train=False)\n",
        "            frames = batch[0].to(device)\n",
        "\n",
        "            # Get embeddings\n",
        "            with torch.no_grad():\n",
        "                bs, s, c, height, width = frames.shape\n",
        "                frames_emb = torch.zeros(s, bs, 512).to(device)\n",
        "                for i in range(s):\n",
        "                    frames_emb[i] = cnn(frames[:, i])\n",
        "        \n",
        "            output = model(frames_emb, batch[1]) \n",
        "\n",
        "            loss_ = criterion(output, frames_emb)\n",
        "            loss_batch = []\n",
        "            for i in range(bs):\n",
        "                loss_batch.append(loss_[:batch[1][i], i].mean())\n",
        "\n",
        "            loss = torch.mean(torch.stack(loss_batch)) \n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / (val_size // batch_size) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJC-1g2sNd1y",
        "outputId": "b8c09e5e-7886-4126-a002-07f1fcc984b7"
      },
      "source": [
        "%%time\n",
        "epoch_loss = 0\n",
        "cnn.eval()\n",
        "\n",
        "for epoch in range(1, max_epochs+1):\n",
        "\n",
        "    batch = generate_batch(images=train_images, \n",
        "                           batch_size=8,  \n",
        "                           train=True, \n",
        "                           n_max=n_max)\n",
        "    frames = batch[0].to(device)\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        bs, s, c, height, width = frames.shape\n",
        "        frames_emb = torch.zeros(s, bs, 512).to(device)\n",
        "        for i in range(s):\n",
        "            frames_emb[i] = cnn(frames[:, i])\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "        \n",
        "    outputs = model(frames_emb, batch[1]) \n",
        "\n",
        "    loss_ = criterion(outputs, frames_emb)\n",
        "    loss_batch = []\n",
        "    for i in range(bs):\n",
        "        loss_batch.append(loss_[:batch[1][i], i].mean())\n",
        "\n",
        "    loss = torch.mean(torch.stack(loss_batch))        \n",
        "    loss.backward()       \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)       \n",
        "    optimizer.step()       \n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    if epoch % every_epochs == 0:\n",
        "        val_loss_0 = evaluate(model, criterion, val_images, batch_size=16)\n",
        "        val_loss_1 = evaluate(model, criterion, val_images_1, batch_size=16)   \n",
        "        print('Epoch : {}'.format(epoch))\n",
        "        print('Train loss : {}'.format(epoch_loss / every_epochs))\n",
        "        print('Val loss 0 : {}'.format(val_loss_0))\n",
        "        print('Val loss 1 : {}'.format(val_loss_1))\n",
        "\n",
        "        epoch_loss = 0\n",
        "        print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 10\n",
            "Train loss : 1.0833514094352723\n",
            "Val loss 0 : 0.6937639117240906\n",
            "Val loss 1 : 0.7765009999275208\n",
            "\n",
            "Epoch : 20\n",
            "Train loss : 0.6999084293842316\n",
            "Val loss 0 : 0.5828014016151428\n",
            "Val loss 1 : 0.7099644343058268\n",
            "\n",
            "Epoch : 30\n",
            "Train loss : 0.6347730994224549\n",
            "Val loss 0 : 0.5504261453946432\n",
            "Val loss 1 : 0.6875843405723572\n",
            "\n",
            "Epoch : 40\n",
            "Train loss : 0.6363241374492645\n",
            "Val loss 0 : 0.5302964846293131\n",
            "Val loss 1 : 0.664537787437439\n",
            "\n",
            "Epoch : 50\n",
            "Train loss : 0.5941894054412842\n",
            "Val loss 0 : 0.5229250391324362\n",
            "Val loss 1 : 0.6609514355659485\n",
            "\n",
            "Epoch : 60\n",
            "Train loss : 0.59133460521698\n",
            "Val loss 0 : 0.517063041528066\n",
            "Val loss 1 : 0.6667480071385702\n",
            "\n",
            "Epoch : 70\n",
            "Train loss : 0.5713346600532532\n",
            "Val loss 0 : 0.5111831823984782\n",
            "Val loss 1 : 0.6603650252024332\n",
            "\n",
            "Epoch : 80\n",
            "Train loss : 0.5509770274162292\n",
            "Val loss 0 : 0.5086961487929026\n",
            "Val loss 1 : 0.6634016434351603\n",
            "\n",
            "Epoch : 90\n",
            "Train loss : 0.5721525251865387\n",
            "Val loss 0 : 0.5098529060681661\n",
            "Val loss 1 : 0.6668801307678223\n",
            "\n",
            "Epoch : 100\n",
            "Train loss : 0.5664644300937652\n",
            "Val loss 0 : 0.508012463649114\n",
            "Val loss 1 : 0.6630566318829855\n",
            "\n",
            "CPU times: user 19min 58s, sys: 2min 49s, total: 22min 47s\n",
            "Wall time: 19min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv4c4P07y9lK"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/v1_RNN_AE.pt')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI7pbMUUnDXZ"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWJRVsTUnWWS"
      },
      "source": [
        "def predict(model, criterion, val_images, batch_size=1):\n",
        "    \n",
        "    model.eval()\n",
        "    cnn.eval()\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    val_size = len(val_images) // batch_size * batch_size\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in range(0, val_size, batch_size):\n",
        "\n",
        "            batch = generate_batch(val_images[i:i+batch_size], batch_size=batch_size, train=False)\n",
        "            frames = batch[0].to(device)\n",
        "            f_len = batch[1]\n",
        "\n",
        "            # Get embeddings\n",
        "            with torch.no_grad():\n",
        "                bs, s, c, height, width = frames.shape\n",
        "                frames_emb = torch.zeros(s, bs, 512).to(device)\n",
        "                for i in range(s):\n",
        "                    frames_emb[i] = cnn(frames[:, i])\n",
        "        \n",
        "            output = model(frames_emb, f_len) \n",
        "\n",
        "            loss_ = criterion(output, frames_emb)\n",
        "            loss_batch = []\n",
        "            for i in range(bs):\n",
        "                loss_batch.append(loss_[:f_len[i], i].mean())\n",
        "\n",
        "            losses += loss_batch \n",
        "        \n",
        "    return losses "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBJACbC76l4c"
      },
      "source": [
        "out_0 = predict(model, criterion, val_images, batch_size=16)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG2NqUQ0ydCP"
      },
      "source": [
        "out_1 = predict(model, criterion, val_images_1, batch_size=16)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22V2lMM11fr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6d552583-aa1d-4e69-a1d7-1a227dcc2aa3"
      },
      "source": [
        "plt.figure()\n",
        "plt.eventplot(out_0, orientation='horizontal', colors='b')\n",
        "plt.eventplot(out_1, orientation='horizontal', colors='r')\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajwxuOSSzW-1"
      },
      "source": [
        "out_0 = predict(model, criterion, val_images[-8:], batch_size=8)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8GudkR2zW-4"
      },
      "source": [
        "out_1 = predict(model, criterion, val_images_1[-8:], batch_size=8)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2z3-WOQHzW-4",
        "outputId": "c94e29bb-49d6-4a84-c581-c193bb56ff7b"
      },
      "source": [
        "plt.figure()\n",
        "plt.eventplot(out_0, orientation='horizontal', colors='b')\n",
        "plt.eventplot(out_1, orientation='horizontal', colors='r')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mMG7yuynWwY"
      },
      "source": [
        "Вывод: В среднем ошибка для атак выше, но четкого разделения не наблюдается."
      ]
    }
  ]
}